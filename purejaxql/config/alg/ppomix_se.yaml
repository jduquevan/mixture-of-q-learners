ALG_NAME: "ppomix_se_cc_mo"
TOTAL_TIMESTEPS: 5e9 # with 4 frame skip correpsnds to 200M frames
TOTAL_TIMESTEPS_DECAY: 5e9 # will be used for decay functions (epsilon and lr)
NUM_ENVS: 128 # parallel environments
NUM_STEPS: 32 # steps per environment in each update
NUM_EPOCHS: 2 # number of epochs per update
NUM_MINIBATCHES: 32 # minibatches per epoch
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
LR: 0.00015
MAX_GRAD_NORM: 10
LR_LINEAR_DECAY: False
GAMMA: 0.99
LAMBDA: 0.65
NUM_AGENTS: 4
CLIP_EPS: 0.1
ENT_COEFF: 0.005
DIVERSITY_COEFF: 0.001
DIV_SAMPLES: 512
DIV_ADV_BETA: 0.25
ACCUM_STEPS: 1

# mixing
MIXING_STEPS: 50            # every N PPO updates do a weighted merge
EMA_ALPHA: 0.05
MIX_TAU: 0.005

# cross agent sample use
CROSS_AGENT_REUSE: true      # turn on/off
CROSS_COEF: 0.75             # weight of cross-agent PPO term
CROSS_SAMPLES: 256            # per agent, per minibatch (drawn from others)
CROSS_RHO_MAX: 10.0    

# env specific, see https://envpool.readthedocs.io/en/latest/env/atari.html
ENV_NAME: "Pong-v5"
ENV_KWARGS: 
  episodic_life: True # lost life -> done, increases sample efficiency, may hurt in some games
  reward_clip: True # reward into -1, 1
  repeat_action_probability: 0. # sticky actions
  frame_skip: 4
  noop_max: 30
  img_width: 84
  img_height: 84

# evaluation
TEST_DURING_TRAINING: True 
TEST_ENVS: 4
EPS_TEST: 0. # 0 for greedy test policy
