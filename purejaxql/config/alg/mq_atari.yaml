ALG_NAME: "mq"

# general training 
NUM_ENVS: 32 # parallel environments
NUM_STEPS: 32 # steps per environment in each update
NUM_EPOCHS: 2 # number of epochs per update
NUM_MINIBATCHES: 32 # minibatches per epoch
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
LR: 0.00025
MAX_GRAD_NORM: 10
LR_LINEAR_DECAY: False
GAMMA: 0.99
LAMBDA: 0.65
EPS_START: 1.
EPS_FINISH: 0.001
REW_SCALE: 1.0

# mq training
mq:
  rounds: 10
# mixture training 
mix:
  NUM_UPDATES: 100
  NUM_AGENTS: 4
  NUM_UPDATES_DECAY: 50 # in how many updates the epsilon drops from eps_start to eps_finish
  BUFFER_PER_AGENT: 128
  SAVE_CHECKPOINT_EVERY: 100 # should be divisible by NUM_UPDATES

# big training
big:
  mid_rounds:
    fill:
      NUM_ENVS: 4
      NUM_TEST_ENVS: 2
    NUM_UPDATES: 100
    NUM_UPDATES_DECAY: 50
    NUM_ENVS: 64
    NUM_TEST_ENVS: 8
  final_round:
    NUM_UPDATES: 5000
    NUM_UPDATES_DECAY: 5000
    NUM_ENVS: 128
    NUM_TEST_ENVS: 16
  BUFFER_SIZE: 512
  SAVE_CHECKPOINT_EVERY: 100 # should be divisible by NUM_UPDATES
    


# env specific (see https://envpool.readthedocs.io/en/latest/env/atari.html)
ENV_NAME: "Pong-v5"
ENV_KWARGS:
  episodic_life: True # lost life -> done, increases sample efficiency, may hurt in some games
  reward_clip: True # reward into -1, 1
  repeat_action_probability: 0. # sticky actions
  frame_skip: 4
  noop_max: 30

# evaluation
TEST_DURING_TRAINING: True 
TEST_ENVS: 4
EPS_TEST: 0. # 0 for greedy test policy
